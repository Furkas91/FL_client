{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fbb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def best_transform(data):\n",
    "    \"\"\"\n",
    "    Функция с ироничным названием и функционал,\n",
    "    целью существования которой является ее\n",
    "    применение к уже обработанным данным, но\n",
    "    для структуры датасета необходима функция\n",
    "    преобразования\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "\n",
    "def mnist_transform():\n",
    "    \"\"\"\n",
    "    Функция предназаченная для примения\n",
    "    в датасете MNIST\n",
    "    :return: функция преобразования\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс, описывающий набор данных для обучения\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, Transform):\n",
    "        self.x = features\n",
    "        self.y = labels\n",
    "        self.transform = Transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.x[index]), self.y[index]\n",
    "\n",
    "\n",
    "def get_mnist_data(df, Transform=best_transform):\n",
    "    \"\"\"\n",
    "    Предобработка,для MNIST датасета\n",
    "    \"\"\"\n",
    "    x_features = df.iloc[:, 1:].values\n",
    "    y_labels = df.label.values\n",
    "    x_features = x_features.reshape(-1, 1, 28, 28)\n",
    "    x_features = np.uint8(x_features)\n",
    "    x_features = torch.from_numpy(x_features)\n",
    "    y_labels = torch.from_numpy(y_labels)\n",
    "    return TrainDataset(x_features, y_labels, Transform)\n",
    "\n",
    "\n",
    "def resize_data(X, y, time_steps=1, step=1):\n",
    "    \"\"\"\n",
    "    Разбиение набора данных на пересекающие фреймы\n",
    "    :param X: матрица признаков\n",
    "    :param y: целевое значение\n",
    "    :param time_steps: размер временного окна\n",
    "    :param step: шаг между началами окон\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: i + time_steps]\n",
    "        Xs.append(v)\n",
    "        ys.append(stats.mode(labels)[0][0])\n",
    "    return np.array(Xs), np.array(ys).reshape(-1)\n",
    "\n",
    "\n",
    "def get_smartiliser_data(df, Transform=best_transform):\n",
    "    \"\"\"\n",
    "    Функция для предобработки смартилайзеровских данных\n",
    "    :param df: DataFrame\n",
    "    :param Transform: функция преобразования\n",
    "    :return: TrainDataset\n",
    "    \"\"\"\n",
    "    # Параметры временного окна\n",
    "    time_steps = 40\n",
    "    step = 10\n",
    "    # Снижение частоты дискретизации\n",
    "    df = df[::2]\n",
    "    # Отбор классов для обучения\n",
    "    df = df[(df.activityMode == 5) | (df.activityMode == 6) | (df.activityMode == 7)]\n",
    "    y_labels = df.activityMode - 5\n",
    "    # Отбор признаков для обучения\n",
    "    scale_columns = ['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ']\n",
    "    df = df[scale_columns]\n",
    "    # Применение надежного скалера\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(df)\n",
    "    df.loc[:, scale_columns] = scaler.transform(df[scale_columns].to_numpy())\n",
    "    x_features = df\n",
    "    # Преобразование временных рядов в набор временных окон\n",
    "    x_features, y_labels = resize_data(x_features, y_labels, time_steps, step)\n",
    "    x_features = x_features.reshape(len(x_features), -1)\n",
    "    # Приведение матрицы признаков к float 32\n",
    "    x_features = np.float32(x_features)\n",
    "    return TrainDataset(x_features, y_labels, Transform)\n",
    "\n",
    "\n",
    "def get_data_loader(path, batch_size, data_name='MNIST'):\n",
    "    \"\"\"\n",
    "    Функция для получения необходимого DataLoader\n",
    "    :param path: путь к CSV файлу\n",
    "    :param batch_size: размер пакета\n",
    "    :param data_name: название набора данных\n",
    "    :return: DataLoader\n",
    "    \"\"\"\n",
    "    # Чтение CSV файла\n",
    "    train_df = pd.read_csv(path)\n",
    "    # Словарь предобработок\n",
    "    preprocces = {\n",
    "        'MNIST': get_mnist_data,\n",
    "        'smartiliser': get_smartiliser_data\n",
    "    }\n",
    "    # Словарь функций преобразования\n",
    "    transform = {\n",
    "        'MNIST': mnist_transform(),\n",
    "        'smartiliser': best_transform,\n",
    "    }\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        preprocces[data_name](train_df, transform[data_name]),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ad65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "loss_functions = {\n",
    "    ''\n",
    "}\n",
    "\n",
    "class MiningSettings:\n",
    "    \"\"\"\n",
    "    Класс для сериализации настроек алгоритма для\n",
    "    обучения нейронной сети\n",
    "    \"\"\"\n",
    "    def __init__(self, algorithm, loss_function, epochs, learning_rate, momentum, batch_size):\n",
    "        self.algorithm = algorithm\n",
    "        self.loss_function = loss_function\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "class UniversalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс универсальной модели\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activations, normalizations):\n",
    "        super(UniversalNet, self).__init__()\n",
    "        self.number_of_layers = len(layers)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activations = activations\n",
    "        self.normalizations = normalizations\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Метод для прямого прохода по нейронной сети\n",
    "        :param x: данные для прохода\n",
    "        :return: данные после прохода\n",
    "        \"\"\"\n",
    "        for i in range(self.number_of_layers):\n",
    "            # Прохождения слоя\n",
    "            x = self.layers[i](x)\n",
    "            # Применение функции активации\n",
    "            x = self.activations[i](x)\n",
    "            # Применение нормализации\n",
    "            x = self.normalizations[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate(test_loader, criterion, net):\n",
    "    \"\"\"\n",
    "    Функция оценки точности модели\n",
    "    :param test_loader: DataLoader\n",
    "    :param criterion: функция потерь\n",
    "    :param net: UniversalNet\n",
    "    :return: точность и ошибка\n",
    "    \"\"\"\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(-1, 40 * 6)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).data.item()\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f})\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def fit(net, train_loader, epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Функция, тренировки модели\n",
    "    :param net: UniversalNet\n",
    "    :param train_loader: DataLoader\n",
    "    :param epochs: количество эпох\n",
    "    :param criterion: функция потерь\n",
    "    :param optimizer: метод оптимизации\n",
    "    :return: UniversalNet\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data = data.view(-1, 40*6)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(loss.data)\n",
    "            #if batch_idx % 100 == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return net\n",
    "\n",
    "\n",
    "def train_evaluate(net, path='D:/FL_client/data/MNIST/train.csv', settings='nothing', data_name='MNIST'):\n",
    "    \"\"\"\n",
    "    Функция для обучения и оценки точности модели\n",
    "    :param net: UniversalNet\n",
    "    :param path: путь к CSV файлу\n",
    "    :param settings: MiningSettings\n",
    "    :param data_name: название набора данных\n",
    "    :return: UniversalNet\n",
    "    \"\"\"\n",
    "    # Загрузка набора данных\n",
    "    train_loader = get_data_loader(path, settings.batch_size, data_name=data_name)\n",
    "    # Создание объекта для метода оптимизации\n",
    "    optimizer = optim.SGD(net.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "    # Создание объекта для функции потерь\n",
    "    criterion = settings.loss_function()\n",
    "    # Вызов функции обучения\n",
    "    net = fit(net, train_loader, settings.epochs, criterion, optimizer)\n",
    "    # Вызов функции оценки точности модели\n",
    "    evaluate(train_loader, criterion, net)\n",
    "    return net, len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2c0845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0206, Accuracy: 103690/106753 (0.971)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        lambda x: x\n",
    "]\n",
    "settings = MiningSettings(\n",
    "        algorithm='SGD',\n",
    "        loss_function=nn.NLLLoss,\n",
    "        epochs=10,\n",
    "        learning_rate=0.1,\n",
    "        momentum=0.9,\n",
    "        batch_size=10\n",
    ")\n",
    "net = UniversalNet(layers, activations, normalizations)\n",
    "net = train_evaluate(net=net, path='D:\\FL_client\\data\\smartilizer\\Video-11-15-40-560.csv', data_name='smartiliser', settings=settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f0d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data_loader('D:\\FL_client\\data\\smartilizer\\Video-16-8-9-212.csv', batch_size=10, data_name='smartiliser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d41bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = settings.loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c4f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1672, Accuracy: 15502/17767 (0.873)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(test, criterion, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66c82fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] Неверно задано имя папки: 'D:\\\\FL_client\\\\data\\\\smartilizer\\\\Video-10-15-12-812.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-da5b7c84f17e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpath_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mpath_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'smartiliser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] Неверно задано имя папки: 'D:\\\\FL_client\\\\data\\\\smartilizer\\\\Video-10-15-12-812.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'D:\\\\FL_client\\\\data\\\\smartilizer'\n",
    "datasets = []\n",
    "for folder in os.listdir(path)[1:]:\n",
    "    path_sub = os.path.join(path, folder)\n",
    "    for file in os.listdir(path_sub):\n",
    "        path_file = os.path.join(path_sub, file)\n",
    "        datasets.append(get_data_loader(path_file, batch_size=10, data_name='smartiliser'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd09203c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.~lock.Video-9-59-58-842.csv#',\n",
       " 'Video-10-15-12-812.csv',\n",
       " 'Video-10-56-50-611.csv',\n",
       " 'Video-11-12-5-821.csv',\n",
       " 'Video-11-15-40-560.csv',\n",
       " 'Video-11-21-6-412.csv',\n",
       " 'Video-11-22-26-986.csv',\n",
       " 'Video-11-35-58-544.csv',\n",
       " 'Video-11-36-16-421.csv',\n",
       " 'Video-11-37-36-962.csv',\n",
       " 'Video-11-5-46-280.csv',\n",
       " 'Video-11-51-13-731.csv',\n",
       " 'Video-11-51-26-279.csv',\n",
       " 'Video-11-52-46-801.csv',\n",
       " 'Video-11-7-16-943.csv',\n",
       " 'Video-12-21-33-548.csv',\n",
       " 'Video-12-36-43-645.csv',\n",
       " 'Video-12-37-13-775.csv',\n",
       " 'Video-12-51-53-673.csv',\n",
       " 'Video-12-52-27-629.csv',\n",
       " 'Video-12-6-23-828.csv',\n",
       " 'Video-13-7-37-524.csv',\n",
       " 'Video-14-21-33-333.csv',\n",
       " 'Video-14-36-46-42.csv',\n",
       " 'Video-14-39-8-297.csv',\n",
       " 'Video-14-48-30-939.csv',\n",
       " 'Video-14-54-21-750.csv',\n",
       " 'Video-15-11-18-475.csv',\n",
       " 'Video-15-16-36-989.csv',\n",
       " 'Video-15-24-39-448.csv',\n",
       " 'Video-15-3-46-79.csv',\n",
       " 'Video-15-37-46-645.csv',\n",
       " 'Video-15-52-59-950.csv',\n",
       " 'Video-15-9-30-558.csv',\n",
       " 'Video-16-10-56-463.csv',\n",
       " 'Video-16-16-44-458.csv',\n",
       " 'Video-16-21-31-998.csv',\n",
       " 'Video-16-26-14-758.csv',\n",
       " 'Video-16-32-0-933.csv',\n",
       " 'Video-16-33-32-460.csv',\n",
       " 'Video-16-36-42-174.csv',\n",
       " 'Video-16-37-58-144.csv',\n",
       " 'Video-16-41-24-433.csv',\n",
       " 'Video-16-47-10-870.csv',\n",
       " 'Video-16-48-48-535.csv',\n",
       " 'Video-16-51-52-142.csv',\n",
       " 'Video-16-56-36-635.csv',\n",
       " 'Video-16-6-17-490.csv',\n",
       " 'Video-16-8-9-212.csv',\n",
       " 'Video-17-14-15-118.csv',\n",
       " 'Video-17-17-30-919.csv',\n",
       " 'Video-17-19-6-527.csv',\n",
       " 'Video-17-2-21-20.csv',\n",
       " 'Video-17-3-57-666.csv',\n",
       " 'Video-17-31-37-911.csv',\n",
       " 'Video-17-32-40-752.csv',\n",
       " 'Video-17-34-45-652.csv',\n",
       " 'Video-17-46-56-752.csv',\n",
       " 'Video-17-47-50-853.csv',\n",
       " 'Video-17-50-0-435.csv',\n",
       " 'Video-18-2-6-571.csv',\n",
       " 'Video-18-20-20-551.csv',\n",
       " 'Video-18-3-0-850.csv',\n",
       " 'Video-18-5-10-437.csv',\n",
       " 'Video-18-54-58-125.csv',\n",
       " 'Video-19-11-10-686.csv',\n",
       " 'Video-19-26-25-45.csv',\n",
       " 'Video-19-41-35-170.csv',\n",
       " 'Video-19-59-2-186.csv',\n",
       " 'Video-20-26-22-794.csv',\n",
       " 'Video-20-41-54-243.csv',\n",
       " 'Video-20-57-3-149.csv',\n",
       " 'Video-21-12-12-318.csv',\n",
       " 'Video-21-27-21-192.csv',\n",
       " 'Video-9-59-58-842.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daae09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
