{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7496125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def best_transform(data):\n",
    "    \"\"\"\n",
    "    Функция с ироничным названием и функционал,\n",
    "    целью существования которой является ее\n",
    "    применение к уже обработанным данным, но\n",
    "    для структуры датасета необходима функция\n",
    "    преобразования\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "\n",
    "def mnist_transform():\n",
    "    \"\"\"\n",
    "    Функция предназаченная для примения\n",
    "    в датасете MNIST\n",
    "    :return: функция преобразования\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс, описывающий набор данных для обучения\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, Transform):\n",
    "        self.x = features\n",
    "        self.y = labels\n",
    "        self.transform = Transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.x[index]), self.y[index]\n",
    "\n",
    "\n",
    "def get_mnist_data(df, Transform=best_transform):\n",
    "    \"\"\"\n",
    "    Предобработка,для MNIST датасета\n",
    "    \"\"\"\n",
    "    x_features = df.iloc[:, 1:].values\n",
    "    y_labels = df.label.values\n",
    "    x_features = x_features.reshape(-1, 1, 28, 28)\n",
    "    x_features = np.uint8(x_features)\n",
    "    x_features = torch.from_numpy(x_features)\n",
    "    y_labels = torch.from_numpy(y_labels)\n",
    "    return TrainDataset(x_features, y_labels, Transform)\n",
    "\n",
    "\n",
    "def resize_data(X, y, time_steps=1, step=1):\n",
    "    \"\"\"\n",
    "    Разбиение набора данных на пересекающие фреймы\n",
    "    :param X: матрица признаков\n",
    "    :param y: целевое значение\n",
    "    :param time_steps: размер временного окна\n",
    "    :param step: шаг между началами окон\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        labels = y.iloc[i: i + time_steps]\n",
    "        Xs.append(v)\n",
    "        ys.append(stats.mode(labels)[0][0])\n",
    "    return np.array(Xs), np.array(ys).reshape(-1)\n",
    "\n",
    "\n",
    "def get_smartiliser_data(df, Transform=best_transform):\n",
    "    \"\"\"\n",
    "    Функция для предобработки смартилайзеровских данных\n",
    "    :param df: DataFrame\n",
    "    :param Transform: функция преобразования\n",
    "    :return: TrainDataset\n",
    "    \"\"\"\n",
    "    # Параметры временного окна\n",
    "    time_steps = 40\n",
    "    step = 10\n",
    "    # Снижение частоты дискретизации\n",
    "    df = df[::2]\n",
    "    # Отбор классов для обучения\n",
    "    df = df[(df.activityMode == 5) | (df.activityMode == 6) | (df.activityMode == 7)]\n",
    "    y_labels = df.activityMode - 5\n",
    "    # Отбор признаков для обучения\n",
    "    scale_columns = ['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ']\n",
    "    df = df[scale_columns]\n",
    "    # Применение надежного скалера\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(df)\n",
    "    df.loc[:, scale_columns] = scaler.transform(df[scale_columns].to_numpy())\n",
    "    x_features = df\n",
    "    # Преобразование временных рядов в набор временных окон\n",
    "    x_features, y_labels = resize_data(x_features, y_labels, time_steps, step)\n",
    "    x_features = x_features.reshape(len(x_features), -1)\n",
    "    # Приведение матрицы признаков к float 32\n",
    "    x_features = np.float32(x_features)\n",
    "    return TrainDataset(x_features, y_labels, Transform)\n",
    "\n",
    "\n",
    "def get_data_loader(path, batch_size, data_name='MNIST'):\n",
    "    \"\"\"\n",
    "    Функция для получения необходимого DataLoader\n",
    "    :param path: путь к CSV файлу\n",
    "    :param batch_size: размер пакета\n",
    "    :param data_name: название набора данных\n",
    "    :return: DataLoader\n",
    "    \"\"\"\n",
    "    # Чтение CSV файла\n",
    "    train_df = pd.read_csv(path)\n",
    "    # Словарь предобработок\n",
    "    preprocces = {\n",
    "        'MNIST': get_mnist_data,\n",
    "        'smartiliser': get_smartiliser_data\n",
    "    }\n",
    "    # Словарь функций преобразования\n",
    "    transform = {\n",
    "        'MNIST': mnist_transform(),\n",
    "        'smartiliser': best_transform,\n",
    "    }\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        preprocces[data_name](train_df, transform[data_name]),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95330885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "loss_functions = {\n",
    "    ''\n",
    "}\n",
    "\n",
    "class MiningSettings:\n",
    "    \"\"\"\n",
    "    Класс для сериализации настроек алгоритма для\n",
    "    обучения нейронной сети\n",
    "    \"\"\"\n",
    "    def __init__(self, algorithm, loss_function, epochs, learning_rate, momentum, batch_size):\n",
    "        self.algorithm = algorithm\n",
    "        self.loss_function = loss_function\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "class UniversalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Класс универсальной модели\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activations, normalizations):\n",
    "        super(UniversalNet, self).__init__()\n",
    "        self.number_of_layers = len(layers)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activations = activations\n",
    "        self.normalizations = normalizations\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Метод для прямого прохода по нейронной сети\n",
    "        :param x: данные для прохода\n",
    "        :return: данные после прохода\n",
    "        \"\"\"\n",
    "        for i in range(self.number_of_layers):\n",
    "            # Прохождения слоя\n",
    "            x = self.layers[i](x)\n",
    "            # Применение функции активации\n",
    "            x = self.activations[i](x)\n",
    "            # Применение нормализации\n",
    "            x = self.normalizations[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate(test_loader, criterion, net):\n",
    "    \"\"\"\n",
    "    Функция оценки точности модели\n",
    "    :param test_loader: DataLoader\n",
    "    :param criterion: функция потерь\n",
    "    :param net: UniversalNet\n",
    "    :return: точность и ошибка\n",
    "    \"\"\"\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(-1, 40 * 6)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).data.item()\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f})\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def fit(net, train_loader, epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Функция, тренировки модели\n",
    "    :param net: UniversalNet\n",
    "    :param train_loader: DataLoader\n",
    "    :param epochs: количество эпох\n",
    "    :param criterion: функция потерь\n",
    "    :param optimizer: метод оптимизации\n",
    "    :return: UniversalNet\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data = data.view(-1, 40*6)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(loss.data)\n",
    "            #if batch_idx % 100 == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return net\n",
    "\n",
    "\n",
    "def train_evaluate(net, path='D:/FL_client/data/MNIST/train.csv', settings='nothing', data_name='MNIST'):\n",
    "    \"\"\"\n",
    "    Функция для обучения и оценки точности модели\n",
    "    :param net: UniversalNet\n",
    "    :param path: путь к CSV файлу\n",
    "    :param settings: MiningSettings\n",
    "    :param data_name: название набора данных\n",
    "    :return: UniversalNet\n",
    "    \"\"\"\n",
    "    # Загрузка набора данных\n",
    "    train_loader = get_data_loader(path, settings.batch_size, data_name=data_name)\n",
    "    # Создание объекта для метода оптимизации\n",
    "    optimizer = optim.SGD(net.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "    # Создание объекта для функции потерь\n",
    "    criterion = settings.loss_function()\n",
    "    # Вызов функции обучения\n",
    "    net = fit(net, train_loader, settings.epochs, criterion, optimizer)\n",
    "    # Вызов функции оценки точности модели\n",
    "    evaluate(train_loader, criterion, net)\n",
    "    return net, len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a829a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0123, Accuracy: 103341/106753 (0.968)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "settings = MiningSettings(\n",
    "        algorithm='SGD',\n",
    "        loss_function=nn.NLLLoss,\n",
    "        epochs=5,\n",
    "        learning_rate=0.1,\n",
    "        momentum=0.9,\n",
    "        batch_size=10\n",
    ")\n",
    "net = UniversalNet(layers, activations, normalizations)\n",
    "net = train_evaluate(net=net, path='D:\\FL_client\\data\\smartilizer\\Video-11-15-40-560.csv', data_name='smartiliser', settings=settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95de55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data_loader('D:\\FL_client\\data\\smartilizer\\Video-16-8-9-212.csv', batch_size=10, data_name='smartiliser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c925d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = settings.loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8078c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0666, Accuracy: 15391/17767 (0.866)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(test, criterion, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a0dac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'D:\\\\FL_client\\\\data\\\\smartilizer'\n",
    "datasets = []\n",
    "for folder in os.listdir(path)[1:6]:\n",
    "    path_file = os.path.join(path, folder)\n",
    "    datasets.append(get_data_loader(path_file, batch_size=10, data_name='smartiliser'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc95a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4512c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# 1 model\n",
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "net_1 = UniversalNet(layers, activations, normalizations)\n",
    "optimizer = optim.SGD(net_1.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "net_1 = fit(net_1, datasets[0], 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698c16f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fl_client\\venv\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 200])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f739c256fc30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mnet_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mnet_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-5a0c742373ec>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(net, train_loader, epochs, criterion, optimizer)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mnet_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\fl_client\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5a0c742373ec>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m# Применение нормализации\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalizations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\fl_client\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\fl_client\\venv\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\fl_client\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2145\u001b[0m         )\n\u001b[0;32m   2146\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m     return torch.batch_norm(\n",
      "\u001b[1;32md:\\fl_client\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2112\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2114\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 200])"
     ]
    }
   ],
   "source": [
    "# 2 model\n",
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "net_2 = UniversalNet(layers, activations, normalizations)\n",
    "optimizer = optim.SGD(net_2.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "net_2 = fit(net_2, datasets[1], 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 model\n",
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "net_3 = UniversalNet(layers, activations, normalizations)\n",
    "optimizer = optim.SGD(net_3.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "net_3 = fit(net_3, datasets[2], 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 model\n",
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "net_4 = UniversalNet(layers, activations, normalizations)\n",
    "optimizer = optim.SGD(net_4.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "net_4 = fit(net_4, datasets[3], 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 model\n",
    "layers = [\n",
    "        nn.Linear(240, 200),\n",
    "        nn.Linear(200, 200),\n",
    "        nn.Linear(200, 100),\n",
    "        nn.Linear(100, 3)\n",
    "]\n",
    "activations = [\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.relu,\n",
    "        F.log_softmax\n",
    "]\n",
    "normalizations = [\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(200),\n",
    "        nn.BatchNorm1d(100),\n",
    "        lambda x: x\n",
    "]\n",
    "net_5 = UniversalNet(layers, activations, normalizations)\n",
    "optimizer = optim.SGD(net_5.parameters(), lr=settings.learning_rate, momentum=settings.momentum)\n",
    "net_5 = fit(net_5, datasets[4], 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3df4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
